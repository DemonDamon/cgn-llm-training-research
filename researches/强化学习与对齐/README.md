# 强化学习与对齐

本目录收集了关于**强化学习与对齐**的技术资料和研究文献。

## 📝 主题简介

研究RLHF、DPO等人类偏好对齐技术和强化学习方法。

## 📚 推荐资料

以下是精选的高质量学习资源：

### 1. 📘 官方文档

- **链接**: [https://aws.amazon.com/cn/what-is/reinforcement-learning-from-human-feedback/](https://aws.amazon.com/cn/what-is/reinforcement-learning-from-human-feedback/)
- **来源**: aws.amazon.com
- **建议**: 点击链接在线阅读

### 2. ✍️ 技术博客

- **链接**: [https://zhuanlan.zhihu.com/p/695990388](https://zhuanlan.zhihu.com/p/695990388)
- **来源**: zhuanlan.zhihu.com
- **建议**: 点击链接在线阅读

### 3. ✍️ 技术博客

- **链接**: [https://zhuanlan.zhihu.com/p/716947703](https://zhuanlan.zhihu.com/p/716947703)
- **来源**: zhuanlan.zhihu.com
- **建议**: 点击链接在线阅读

### 4. 🤗 HuggingFace

- **链接**: [https://huggingface.co/blog/ariG23498/rlhf-to-dpo](https://huggingface.co/blog/ariG23498/rlhf-to-dpo)
- **来源**: huggingface.co
- **建议**: 点击链接在线阅读

### 5. 🔗 技术文章

- **链接**: [https://snorkel.ai/blog/llm-alignment-techniques-4-post-training-approaches/](https://snorkel.ai/blog/llm-alignment-techniques-4-post-training-approaches/)
- **来源**: snorkel.ai
- **建议**: 点击链接在线阅读

## 💡 学习建议

1. **循序渐进**: 建议按照资料顺序学习，从基础概念到高级应用
2. **实践结合**: 理论学习的同时，建议结合实际项目进行实践
3. **持续更新**: 大模型技术快速发展，建议关注最新研究进展

## 📊 资料统计

- 资料总数: 5
- 更新时间: 2025-12-01

---

*注: 本目录资料来源于公开网络，仅供学习研究使用。如有侵权请联系删除。*
